.. _beforepushing:

Mandatory checks
================

This page illustrates all the information regarding what to check before and
while you push your modifications.

The development of this software is monitored by a Continuous Integration (CI)
pipeline that takes care of all checks (some using external services).

.. contents:: Index
    :local:
    :depth: 2

Documentation
-------------

Each Pull Request (PR) has to have its own documentation updates (if any),
according to what are the changes to be merged into master.

The documentation is generated by `Sphinx <https://www.sphinx-doc.org/en/master/>`__
with `reStructuredText <https://docutils.sourceforge.io/rst.html>`__ syntax.

To build and check your documentation locally,

- cd ``docs``
- for big changes (or just to be sure), ``rm api/* && make clean && make html``
- for small changes, ``make html``

The built documentation will be stored under ``docs/_build/html`` from which you
can open ``index.html`` with your favorite browser.

You will have to fix any warning that appears during documentation building,
because the documentation also runs on `readthedocs <https://readthedocs.org/>`__
with an option to treat warnings as errors.

Testing
-------

All testing code is called by issuing the ``pytest`` command.

This command can be called from any place within the cloned repository and it
will always run from the root directory of the project.

For debugging purposes you can add the ``-s`` option which will allow to 
visualise any ``print`` statement within the test module(s).

Testing is automatically triggered by the CI every time a new
pull-request is pushed to the repository, and its correct
execution is one of the mandatory condition for merging.

Unit tests
^^^^^^^^^^

You can follow 
`these guidelines <https://cta-observatory.github.io/ctapipe/development/code-guidelines.html#unit-tests>`__
to understand what a unit-test is supposed to do.

.. note::
  This is a maintenance activity which has being long overdue and we need
  manpower for it, so if you have experience on this or you want to contribute
  please feel free to do so.

  For more information on how to contribute to this effort check
  `this issue <https://github.com/cta-observatory/protopipe/issues/69>`__.

Being *protopipe* based on *ctapipe*, all the tools imported from the latter
have been already tested and approved (*protopipe* uses always a version of
*ctapipe* which has been released on the Anaconda framework).
Same for *pyirf*.

.. warning::
  This is not true for,

  - hard-coded parts that had to be modified in anticipation of code migration,
  - *protopipe* functions themselves (which will eventually migrate to *ctapipe*)

  Regarding the first point: given the difference in versions between the
  imported *ctapipe* and its development version, sometimes it's possible that, in
  order to code a new feature, this has to be pull-requested to *ctapipe* and at
  the same time hardcoded in *protopipe*, until the new version of *ctapipe* is released.

Integration tests
^^^^^^^^^^^^^^^^^

These are neither unit-tests nor benchmarks, but rather functions that test
whole functionalities and not just single API functions.

In the case of the pipeline, such functionalities are the scripts/tools
that make up its workflow.

.. note::
  For more information on how to contribute to this effort check
  `this issue <https://github.com/cta-observatory/protopipe/issues/70>`__.

The integration tests are defined in the dedicated module ``pipeline/scripts/tests/test_pipeline.py``
and start from test simtel files stored on a CC-IN2P3 dataserver.  

The test data is diffuse data from the Prod3b baseline simulations of both 
CTAN and CTAS produced with the following Corsika settings,

- gammas, ``NSHOW=10 ESLOPE=-2.0 EMIN=10 EMAX=20 NSCAT=1 CSCAT=200 VIEWCONE=3``
- protons, ``NSHOW=10 ESLOPE=-2.0 EMIN=100 EMAX=200 NSCAT=1 CSCAT=200 VIEWCONE=3``
- electrons, ``NSHOW=10 ESLOPE=-2.0 EMIN=10 EMAX=20 NSCAT=1 CSCAT=200 VIEWCONE=3``

and it is analysed using the same workflow as in a standard full-scale analysis.

Benchmarks
----------

.. toctree::
   :hidden:

   benchmarks/TRAINING/benchmarks_DL1_calibration
   benchmarks/TRAINING/benchmarks_DL1_image-cleaning
   benchmarks/TRAINING/benchmarks_DL1_image-cleaning-with-true-phes
   benchmarks/TRAINING/benchmarks_DL1_DirectionLUT
   benchmarks/TRAINING/benchmarks_DL2_direction-reconstruction
   benchmarks/TRAINING/benchmarks_DL2_to_energy-estimation
   benchmarks/TRAINING/benchmarks_DL2_EnergyLUT
   benchmarks/TRAINING/benchmarks_DL2_to_classification
   benchmarks/MODELS/benchmarks_MODELS_energy
   benchmarks/MODELS/benchmarks_MODELS_classification
   benchmarks/DL2/benchmarks_DL2_particle-classification
   benchmarks/DL2/benchmarks_DL2_direction-reconstruction
   benchmarks/DL3/benchmarks_DL3_cuts_optimization
   benchmarks/DL3/benchmarks_DL3_IRFs_and_sensitivity
   benchmarks/DL3/overall_performance_plot_CTA

This documentation hosts a series of notebooks used for benchmarking.

Their contents follow the development triggered by the ongoing
comparison between protopipe and CTA-MARS (see
`this issue <https://github.com/cta-observatory/protopipe/issues/24>`__ and
references therein for a summary).

The following tables summarize the analyses currently performed and planned with
*protopipe* to evaluate its performance and that of the observatory.
Links are provided for the simtel files lists and reference data from historical
pipelines, which is used throughout the notebooks for comparison and performance monitoring.

.. note::
  Also this is a heavy-load activity which could be shared.

  If you're interested in analysing any of the simulations listed here or you
  want to propose new ones you are more than welcome to do it and contribute
  to the benchmarking following the set of prescriptions described in
  this documentation.

.. list-table:: **PROD 3B**
   :widths: 25 25 25 25 25 25
   :header-rows: 1

   * - Site
     - Array
     - Zenith
     - Azimuth
     - simtel
     - CTAMARS
   * - North (La Palma)
     - baseline
     - 20°
     - 180°
     - `link <https://forge.in2p3.fr/attachments/download/63177/CTA-N_from_South.zip>`__
     - `link <https://forge.in2p3.fr/projects/step-by-step-reference-mars-analysis/wiki>`__
   * - South (Paranal desert)
     - baseline
     - 20°
     - 180°
     - ...
     - ...

.. list-table:: **PROD 5**
  :widths: 25 25 25 25 25 25
  :header-rows: 1

  * - Site
    - Array
    - Zenith
    - Azimuth
    - simtel
    - EventDisplay
  * - North (La Palma)
    - baseline
    - 20°
    - 180°
    - ...
    - ...
  * - South (Paranal desert)
    - baseline
    - 20°
    - 180°
    - ...
    - ...

In the documentation we show only the full-pipeline performance results from
the latest release.
It is suggested to open the notebooks with ``jupyter lab``
from their location at ``docs/contribute/benchmarks``.

The benchmarks are organised as follows,

- TRAINING

  * `Calibration <benchmarks/TRAINING/benchmarks_DL1_calibration.ipynb>`__ | *benchmarks_DL1_calibration.ipynb*
  * `Image cleaning <benchmarks/TRAINING/benchmarks_DL1_image-cleaning.ipynb>`__ | *benchmarks_DL1_image-cleaning.ipynb*
  * `Parameters vs true information <benchmarks/TRAINING/benchmarks_DL1_image-cleaning-with-true-phes.ipynb>`__ | *benchmarks_DL1_image-cleaning-with-true-phes.ipynb*
  * `Direction Look-Up Tables <benchmarks/TRAINING/benchmarks_DL1_DirectionLUT.ipynb>`__ | *benchmarks_DL1_DirectionLUT.ipynb*
  * `Direction reconstruction <benchmarks/TRAINING/benchmarks_DL2_direction-reconstruction.ipynb>`__ | *benchmarks_DL2_direction-reconstruction.ipynb*
  * `to energy estimator <benchmarks/TRAINING/benchmarks_DL2_to_energy-estimation.ipynb>`__ | *benchmarks_DL2_to_energy-estimation.ipynb*
  * `Energy Look-Up Tables <benchmarks/TRAINING/benchmarks_DL2_EnergyLUT.ipynb>`__ | *benchmarks_DL2_EnergyLUT.ipynb*
  * `to classifier <benchmarks/TRAINING/benchmarks_DL2_to_classification.ipynb>`__ | *benchmarks_DL2_to_classification.ipynb*

In particular:

  * calibration requires *ctapipe* DL1a output (images without parameters),
  * all image cleaning and direction reconstruction benchmarks use *protopipe*
    TRAINING data **without** estimated energy,
  * all benchmarks for the energy estimator use *protopipe* TRAINING data **without** estimated energy,
  * benchmarks for the classifier use *protopipe* TRAINING data **with** energy as the only estimated DL2b parameter.

- MODELS

These performances are obtained from a *test* portion of the TRAINING data,

  * `Energy <benchmarks/MODELS/benchmarks_MODELS_energy.ipynb>`__ | *benchmarks_MODELS_energy.ipynb*
  * `Particle type <benchmarks/MODELS/benchmarks_MODELS_classification.ipynb>`__ | *benchmarks_MODELS_classification.ipynb*

- DL2

  * `Particle classification <benchmarks/DL2/benchmarks_DL2_particle-classification.ipynb>`__ | *benchmarks_DL2_particle-classification.ipynb*
  * `Direction reconstruction <benchmarks/DL2/benchmarks_DL2_direction-reconstruction.ipynb>`__ | *benchmarks_DL2_direction-reconstruction.ipynb*

- DL3

  * `Cuts optimization <benchmarks/DL3/benchmarks_DL3_cuts_optimization.ipynb>`__ | *benchmarks_DL3_cuts_optimization.ipynb*
  * `Instrument Response Functions and sensitivity (internal) <https://forge.in2p3.fr/projects/benchmarks-reference-analysis/wiki/Protopipe_performance_data>`__ | *benchmarks_DL3_IRFs_and_sensitivity*
  * `Performance poster (internal) <https://forge.in2p3.fr/projects/benchmarks-reference-analysis/wiki/Protopipe_performance_data>`__ | *overall_performance_plot_CTA.ipynb*  

The DL3 folder contains also the CTA requirements, while the ASWG performance
data is left to the user, being internal.

.. note::
  This part of *protopipe* is not meant to be kept here in the end, in order to
  avoid divergences with
  `ctaplot <https://github.com/cta-observatory/ctaplot>`__ and
  `cta-benchmarks <https://github.com/cta-observatory/cta-benchmarks>`__.

  Plots should be properly migrated to *ctaplot* and the single pipeline steps
  will ported to cta-benchmarks after the pipeline has been refactored using
  *ctapipe*'s stage tools.

.. note::
  The storage of static versions of the benchmarks in this documentation is temporary.
  It is planned to run such benchmarks from the CTAO GitLab runner's CI via an
  external IN2P3 data server in which test data, benchmarks and documentation
  will be stored.

  For the moment the purpose of these tools is to help
  developers and testers to check if their changes improve or degrade
  previous performances stored in the documentation.
